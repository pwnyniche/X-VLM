{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/xvlm/lib/python3.9/site-packages/torchvision/transforms/transforms.py:834: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/xvlm/lib/python3.9/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dataset import create_dataset, create_sampler, create_loader\n",
    "from models.model_nlvr import XVLM\n",
    "from models.model_cosmos import XVLM\n",
    "from models import XVLMBase, build_mlp, load_pretrained\n",
    "config = {'train_file': ['data/finetune/nlvr_train.json'], 'val_file': ['data/finetune/nlvr_dev.json'], \n",
    "'test_file': ['data/finetune/nlvr_test.json'], 'image_root': 'images/nlvr2/', \n",
    "'vision_config': 'configs/config_swinB_384.json', 'use_clip_vit': False, \n",
    "'use_swin': True, 'image_res': 384, 'patch_size': 32, 'use_roberta': False, \n",
    "'text_config': 'configs/config_bert.json', 'text_encoder': 'data/bert-base-uncased', \n",
    "'batch_size': 5, 'optimizer': {'opt': 'adamW', 'lr': 3e-05, 'weight_decay': 0.01, 'lr_mult': 2}, 'schedular': {'sched': 'linear', 'lr': 3e-05, 'epochs': 10, 'num_warmup_steps': 0.1}}\n",
    "# config_vqa = {'train_file': ['data/finetune/vqa_train.json', 'data/finetune/vqa_val.json', 'data/finetune/vg_qa.json'], 'test_file': ['data/finetune/vqa_test.json'], 'answer_list': 'data/finetune/answer_list.json', 'vqa_root': 'images/coco/', 'vg_root': 'images/visualgenome/', 'vision_config': 'configs/config_swinB_384.json', 'use_clip_vit': False, 'use_swin': True, 'image_res': 384, 'patch_size': 32, 'use_roberta': False, 'text_config': 'configs/config_bert.json', 'text_encoder': 'data/bert-base-uncased', 'num_dec_layers': 6, 'batch_size_train': 24, 'batch_size_test': 32, 'max_tokens': 40, 'k_test': 128, 'optimizer': {'opt': 'adamW', 'lr': 5e-05, 'weight_decay': 0.01, 'lr_mult': 2}, 'schedular': {'sched': 'linear', 'lr': 5e-05, 'epochs': 10, 'num_warmup_steps': 0.1}, 'start_eval': 7}\n",
    "config = {'train_file': ['data/finetune/cosmos_train.json'], 'val_file': ['data/finetune/cosmos_val.json'], \n",
    "'test_file': ['data/finetune/cosmos_test.json'], 'image_root': 'images/cosmos/', \n",
    "'vision_config': 'configs/config_swinB_384.json', 'use_clip_vit': False, \n",
    "'use_swin': True, 'image_res': 384, 'patch_size': 32, 'use_roberta': False, \n",
    "'text_config': 'configs/config_bert.json', 'text_encoder': 'data/bert-base-uncased', \n",
    "'batch_size': 5, 'optimizer': {'opt': 'adamW', 'lr': 3e-05, 'weight_decay': 0.01, 'lr_mult': 2}, 'schedular': {'sched': 'linear', 'lr': 3e-05, 'epochs': 10, 'num_warmup_steps': 0.1}}\n",
    "train_dataset, val_dataset, test_dataset = create_dataset('cosmos', config);\n",
    "datasets = [train_dataset, val_dataset, test_dataset]\n",
    "# config_vqa['pad_token_id'] = train_dataset.pad_token_id\n",
    "# config_vqa['eos'] = train_dataset.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_loader(datasets, [None, None, None], batch_size=[config['batch_size']] * 3,\n",
    "                                                          num_workers=[4, 4, 4], is_trains=[True, False, False],\n",
    "                                                          collate_fns=[None, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1700"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading pretrained vision encoder\n",
      "### Loading pretrained text encoder\n",
      "load checkpoint from output/cosmos/checkpoint_best.pth\n",
      "missing_keys:  []\n",
      "unexpected_keys:  []\n"
     ]
    }
   ],
   "source": [
    "model = XVLM(config=config)\n",
    "model.load_pretrained('output/cosmos/checkpoint_best.pth', config, is_eval=False)\n",
    "model = model.to('cuda');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tokenization_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(config['text_encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5506"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s='''\n",
    "Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside. Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside. Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside.\n",
    "Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside. Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside. Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside.\n",
    "Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside. Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside. Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside.\n",
    "Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside. Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside. Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside.\n",
    "Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside. Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside. Twenty-five years Dana had been waiting. She tried to be patient during that time but she hadn't always managed to be as patient as she'd like. But today the opportunity had finally come. The thing she always imagined would make her the happiest person in the world was about to happen. She didn't know why at this specific time she all of a sudden felt sick inside.\n",
    "'''\n",
    "#s='Hello my name is funny hell and I really hate you'\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:39<00:00,  8.64it/s]\n"
     ]
    }
   ],
   "source": [
    "device='cuda'\n",
    "prediction = []\n",
    "for image, caption1, caption2, targets in tqdm(test_loader):\n",
    "    image, targets = image.to(device), targets.to(device)   \n",
    "    text_inputs_1 = tokenizer(caption1, padding='longest', return_tensors=\"pt\",max_length=512, truncation=True).to(device)\n",
    "    text_inputs_2 = tokenizer(caption2, padding='longest', return_tensors=\"pt\",max_length=512, truncation=True).to(device)\n",
    "\n",
    "    loss = model(image, text_inputs_1.input_ids, text_inputs_1.attention_mask, \n",
    "    text_inputs_2.input_ids, text_inputs_2.attention_mask, \n",
    "    targets=targets, train=False)\n",
    "    prediction += loss.argmax(dim=1).tolist()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/340 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████| 340/340 [00:08<00:00, 37.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['caption1','caption2','target'])\n",
    "for image, caption1, caption2, targets in tqdm(test_loader):\n",
    "    # df = df.append([caption1, caption2, targets])\n",
    "    for c1,c2,t in zip(caption1, caption2, targets):\n",
    "        df.loc[len(df)] = [c1,c2,t.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predict'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict    0    1\n",
      "target           \n",
      "0        768   82\n",
      "1        522  328\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(df['target'], df['predict'], rownames=['target'], colnames=['predict'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3        True\n",
       "4       False\n",
       "        ...  \n",
       "1695     True\n",
       "1696    False\n",
       "1697    False\n",
       "1698    False\n",
       "1699     True\n",
       "Length: 1700, dtype: bool"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "entail = pa.ipc.RecordBatchFileReader(\n",
    "    pa.memory_map(f\"entail.arrow\", \"r\")\n",
    ").read_all().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entail'] = entail['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption1</th>\n",
       "      <th>caption2</th>\n",
       "      <th>target</th>\n",
       "      <th>predict</th>\n",
       "      <th>entail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actor, musician, director and devoted follower of Christ, PERSON, has been stricken with an extremely rare form of what can only be described as a biblical disease.</td>\n",
       "      <td>A shocking report about the former child actor PERSON was just another hoax from a well-known purveyor of satire.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A bird (variously described as a goose or a duck) helping to keep a puppy warm</td>\n",
       "      <td>It isn’t unprecedented for birds and dogs to strike up friendships, thus claim is false</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A photograph shows PERSON, a white woman who was killed by a black police officer in GPE.</td>\n",
       "      <td>A viral meme got key facts wrong while attempting to make a point about police violence and racial injustice in the GPE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A video shows people in GPE tearing down a CARDINALG tower in an attempt to stop the spread of COVID-19 coronavirus disease.</td>\n",
       "      <td>The video shows protesters in GPE in DATE (before the COVID-19 pandemic) tearing down a surveillance tower.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ORG warned that Asteroid 52768 (DATE) could hit LOC in DATE and cause catastrophic danger.</td>\n",
       "      <td>This image of asteroid PRODUCT, taken by ORG's ORG on DATE, shows the north polar region highlighting the major physiographic features of the northern hemisphere; the saddle seen at the bottom.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>A photo shows PERSON laughing after PERSON \"pranked\" her by disguising himself as a palace guard.</td>\n",
       "      <td>The prince and queen shortly before a ceremonial review of ORG at FAC.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>ORG secretly bought the “WORK_OF_ART,” PERSON, from a reclusive GPE businessman, PERSON, for MONEY in DATE.</td>\n",
       "      <td>The reports related to Mark purchasing a yacht are completely inaccurate as he did not purchase a yacht.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>A video shows a naked child using bedsheets to escape from an upper story of FAC.</td>\n",
       "      <td>The naked man was actually escaping from Princess Eleanor’s (PERSON) chambers</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>President PERSON trademarked the name 'WORK_OF_ART' and receives royalties every time the term is used.</td>\n",
       "      <td>There was no truth that PERSON family MONEY with ORG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>We are confident that our reporting will stand up, said PERSON, the editor in chief of ORG.</td>\n",
       "      <td>ORG Editor-in-Chief PERSON poses for a picture in his office in the newsroom at ORG headquarters.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                  caption1  \\\n",
       "3     Actor, musician, director and devoted follower of Christ, PERSON, has been stricken with an extremely rare form of what can only be described as a biblical disease.   \n",
       "6                                                                                           A bird (variously described as a goose or a duck) helping to keep a puppy warm   \n",
       "7                                                                                A photograph shows PERSON, a white woman who was killed by a black police officer in GPE.   \n",
       "23                                            A video shows people in GPE tearing down a CARDINALG tower in an attempt to stop the spread of COVID-19 coronavirus disease.   \n",
       "25                                                                              ORG warned that Asteroid 52768 (DATE) could hit LOC in DATE and cause catastrophic danger.   \n",
       "...                                                                                                                                                                    ...   \n",
       "1686                                                                     A photo shows PERSON laughing after PERSON \"pranked\" her by disguising himself as a palace guard.   \n",
       "1688                                                           ORG secretly bought the “WORK_OF_ART,” PERSON, from a reclusive GPE businessman, PERSON, for MONEY in DATE.   \n",
       "1692                                                                                     A video shows a naked child using bedsheets to escape from an upper story of FAC.   \n",
       "1695                                                               President PERSON trademarked the name 'WORK_OF_ART' and receives royalties every time the term is used.   \n",
       "1699                                                                           We are confident that our reporting will stand up, said PERSON, the editor in chief of ORG.   \n",
       "\n",
       "                                                                                                                                                                                               caption2  \\\n",
       "3                                                                                     A shocking report about the former child actor PERSON was just another hoax from a well-known purveyor of satire.   \n",
       "6                                                                                                               It isn’t unprecedented for birds and dogs to strike up friendships, thus claim is false   \n",
       "7                                                                               A viral meme got key facts wrong while attempting to make a point about police violence and racial injustice in the GPE   \n",
       "23                                                                                          The video shows protesters in GPE in DATE (before the COVID-19 pandemic) tearing down a surveillance tower.   \n",
       "25    This image of asteroid PRODUCT, taken by ORG's ORG on DATE, shows the north polar region highlighting the major physiographic features of the northern hemisphere; the saddle seen at the bottom.   \n",
       "...                                                                                                                                                                                                 ...   \n",
       "1686                                                                                                                             The prince and queen shortly before a ceremonial review of ORG at FAC.   \n",
       "1688                                                                                           The reports related to Mark purchasing a yacht are completely inaccurate as he did not purchase a yacht.   \n",
       "1692                                                                                                                      The naked man was actually escaping from Princess Eleanor’s (PERSON) chambers   \n",
       "1695                                                                                                                                               There was no truth that PERSON family MONEY with ORG   \n",
       "1699                                                                                                  ORG Editor-in-Chief PERSON poses for a picture in his office in the newsroom at ORG headquarters.   \n",
       "\n",
       "      target  predict         entail  \n",
       "3          1        0        neutral  \n",
       "6          1        0        neutral  \n",
       "7          1        0        neutral  \n",
       "23         1        0        neutral  \n",
       "25         1        0        neutral  \n",
       "...      ...      ...            ...  \n",
       "1686       1        0  contradiction  \n",
       "1688       1        0  contradiction  \n",
       "1692       1        0  contradiction  \n",
       "1695       1        0        neutral  \n",
       "1699       1        0        neutral  \n",
       "\n",
       "[522 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['target']==1) & (df['predict']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 2, 5, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.tensor([5,2]), torch.tensor([5,2])],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0831, -0.3668],\n",
       "        [ 0.0402, -0.0914],\n",
       "        [-0.0596, -0.3729],\n",
       "        [-0.2012, -0.3863],\n",
       "        [-0.0547, -0.1801]], device='cuda:1', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/thesis/X-VLM/dataset.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.29.64.76/root/thesis/X-VLM/dataset.ipynb#ch0000006vscode-remote?line=0'>1</a>\u001b[0m device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.29.64.76/root/thesis/X-VLM/dataset.ipynb#ch0000006vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m (image0, image1, text, targets) \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.29.64.76/root/thesis/X-VLM/dataset.ipynb#ch0000006vscode-remote?line=2'>3</a>\u001b[0m     images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([image0, image1], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.29.64.76/root/thesis/X-VLM/dataset.ipynb#ch0000006vscode-remote?line=3'>4</a>\u001b[0m     images, targets \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)   \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.29.64.76/root/thesis/X-VLM/dataset.ipynb#ch0000006vscode-remote?line=5'>6</a>\u001b[0m     text_inputs \u001b[39m=\u001b[39m tokenizer(text, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlongest\u001b[39m\u001b[39m'\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device='cuda'\n",
    "for (image0, image1, text, targets) in train_loader:\n",
    "    images = torch.cat([image0, image1], dim=0)\n",
    "    images, targets = images.to(device), targets.to(device)   \n",
    "    \n",
    "    text_inputs = tokenizer(text, padding='longest', return_tensors=\"pt\").to(device)  \n",
    "    \n",
    "    loss = model(images, text_inputs.input_ids, text_inputs.attention_mask, targets=targets, train=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.xbert import BertConfig, BertForMaskedLM, BertModel\n",
    "config_text = BertConfig.from_json_file(config['text_config'])\n",
    "text_encoder = BertModel(config=config_text, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds, image_atts = model.get_vision_embeds(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 145, 1024]), torch.Size([10, 145]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeds.shape, image_atts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds, image_atts = model.get_vision_embeds(images)\n",
    "image0_embeds, image1_embeds = torch.split(image_embeds, targets.size(0))\n",
    "(text_ids, text_atts) = text_inputs.input_ids, text_inputs.attention_mask\n",
    "r = model.get_cross_embeds([image0_embeds, image1_embeds], \n",
    "    [image_atts[:image0_embeds.size(0)], image_atts[image0_embeds.size(0):]],\n",
    "    text_ids=text_ids, text_atts=text_atts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/thesis/X-VLM/dataset.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.29.64.76/root/thesis/X-VLM/dataset.ipynb#ch0000016vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# text_encoder, init = build_text_encoder(config, vision_width=145,  config_text=config_text)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.29.64.76/root/thesis/X-VLM/dataset.ipynb#ch0000016vscode-remote?line=1'>2</a>\u001b[0m text_encoder \u001b[39m=\u001b[39m BertModel(config\u001b[39m=\u001b[39mconfig_text, add_pooling_layer\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertModel' is not defined"
     ]
    }
   ],
   "source": [
    "text_encoder, init = build_text_encoder(config, vision_width=145,  config_text=config_text)\n",
    "#text_encoder = BertModel(config=config_text, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 19]),\n",
       " torch.Size([5, 19]),\n",
       " torch.Size([10, 145, 1024]),\n",
       " torch.Size([10, 145]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ids.shape, text_atts.shape, image_embeds.shape, image_atts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/root/thesis/X-VLM/dataset.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.29.64.76/root/thesis/X-VLM/dataset.ipynb#ch0000044vscode-remote?line=0'>1</a>\u001b[0m b \u001b[39m=\u001b[39m text_encoder(text_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.29.64.76/root/thesis/X-VLM/dataset.ipynb#ch0000044vscode-remote?line=1'>2</a>\u001b[0m             attention_mask\u001b[39m=\u001b[39;49m[text_atts,text_atts],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.29.64.76/root/thesis/X-VLM/dataset.ipynb#ch0000044vscode-remote?line=2'>3</a>\u001b[0m             encoder_hidden_states\u001b[39m=\u001b[39;49m[image0_embeds, image1_embeds],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.29.64.76/root/thesis/X-VLM/dataset.ipynb#ch0000044vscode-remote?line=3'>4</a>\u001b[0m             encoder_attention_mask\u001b[39m=\u001b[39;49m[image_atts[:image0_embeds\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m)], image_atts[image0_embeds\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m):]],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.29.64.76/root/thesis/X-VLM/dataset.ipynb#ch0000044vscode-remote?line=4'>5</a>\u001b[0m             return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,)\n",
      "File \u001b[0;32m~/anaconda3/envs/xvlm/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/xvlm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/xvlm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/xvlm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/xvlm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///root/anaconda3/envs/xvlm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/xvlm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/xvlm/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/thesis/X-VLM/models/xbert.py:1018\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, is_decoder, mode)\u001b[0m\n\u001b[1;32m   <a href='file:///root/thesis/X-VLM/models/xbert.py?line=1013'>1014</a>\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m   <a href='file:///root/thesis/X-VLM/models/xbert.py?line=1015'>1016</a>\u001b[0m \u001b[39m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/thesis/X-VLM/models/xbert.py?line=1016'>1017</a>\u001b[0m \u001b[39m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///root/thesis/X-VLM/models/xbert.py?line=1017'>1018</a>\u001b[0m extended_attention_mask: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_extended_attention_mask(attention_mask, input_shape, \n\u001b[1;32m   <a href='file:///root/thesis/X-VLM/models/xbert.py?line=1018'>1019</a>\u001b[0m                                                                          device, is_decoder)\n\u001b[1;32m   <a href='file:///root/thesis/X-VLM/models/xbert.py?line=1020'>1021</a>\u001b[0m \u001b[39m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/thesis/X-VLM/models/xbert.py?line=1021'>1022</a>\u001b[0m \u001b[39m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/thesis/X-VLM/models/xbert.py?line=1022'>1023</a>\u001b[0m \u001b[39mif\u001b[39;00m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/thesis/X-VLM/models/xbert.py:898\u001b[0m, in \u001b[0;36mBertModel.get_extended_attention_mask\u001b[0;34m(self, attention_mask, input_shape, device, is_decoder)\u001b[0m\n\u001b[1;32m    <a href='file:///root/thesis/X-VLM/models/xbert.py?line=881'>882</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///root/thesis/X-VLM/models/xbert.py?line=882'>883</a>\u001b[0m \u001b[39mMakes broadcastable attention and causal masks so that future and masked tokens are ignored.\u001b[39;00m\n\u001b[1;32m    <a href='file:///root/thesis/X-VLM/models/xbert.py?line=883'>884</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///root/thesis/X-VLM/models/xbert.py?line=893'>894</a>\u001b[0m \u001b[39m    :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///root/thesis/X-VLM/models/xbert.py?line=894'>895</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///root/thesis/X-VLM/models/xbert.py?line=895'>896</a>\u001b[0m \u001b[39m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    <a href='file:///root/thesis/X-VLM/models/xbert.py?line=896'>897</a>\u001b[0m \u001b[39m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///root/thesis/X-VLM/models/xbert.py?line=897'>898</a>\u001b[0m \u001b[39mif\u001b[39;00m attention_mask\u001b[39m.\u001b[39;49mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    <a href='file:///root/thesis/X-VLM/models/xbert.py?line=898'>899</a>\u001b[0m     extended_attention_mask \u001b[39m=\u001b[39m attention_mask[:, \u001b[39mNone\u001b[39;00m, :, :]\n\u001b[1;32m    <a href='file:///root/thesis/X-VLM/models/xbert.py?line=899'>900</a>\u001b[0m \u001b[39melif\u001b[39;00m attention_mask\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    <a href='file:///root/thesis/X-VLM/models/xbert.py?line=900'>901</a>\u001b[0m     \u001b[39m# Provided a padding mask of dimensions [batch_size, seq_length]\u001b[39;00m\n\u001b[1;32m    <a href='file:///root/thesis/X-VLM/models/xbert.py?line=901'>902</a>\u001b[0m     \u001b[39m# - if the model is a decoder, apply a causal mask in addition to the padding mask\u001b[39;00m\n\u001b[1;32m    <a href='file:///root/thesis/X-VLM/models/xbert.py?line=902'>903</a>\u001b[0m     \u001b[39m# - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "b = text_encoder(text_ids,\n",
    "            attention_mask=text_atts,\n",
    "            encoder_hidden_states=[image0_embeds, image1_embeds],\n",
    "            encoder_attention_mask=[image_atts[:image0_embeds.size(0)], image_atts[image0_embeds.size(0):]],\n",
    "            return_dict=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 19, 768])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43a2aefd9a4740e0597b3489c1c78fca90cd0ffb50c6fc4e233b22791010a45e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('xvlm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
