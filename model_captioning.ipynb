{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import create_dataset, create_sampler, create_loader\n",
    "from models.model_bbox import XVLM as XVLM_bbox\n",
    "from models.model_captioning import XVLM as XVLM_caption\n",
    "\n",
    "config = {'train_file': ['data/finetune/cosmos_caption_train.json'], 'val_file': ['data/finetune/cosmos_caption_val.json'], \n",
    "'test_file': ['data/finetune/cosmos_caption_val.json'], 'image_root': 'images/cosmos/', \n",
    "'vision_config': 'configs/config_swinB_384.json', 'use_clip_vit': False, 'use_swin': True, \n",
    "'image_res': 384, 'patch_size': 32, 'use_roberta': False, 'text_config': 'configs/config_bert.json', \n",
    "'text_encoder': 'data/bert-base-uncased', 'num_dec_layers': 6, 'batch_size_train': 5, \n",
    "'batch_size_test': 32, 'max_tokens': 40, 'label_smoothing': 0.1, 'max_length': 20, 'min_length': 5, \n",
    "'num_beams': 3, 'prompt': 'a picture of ', 'optimizer': {'opt': 'adamW', 'lr': 1e-05, \n",
    "'weight_decay': 0.01, 'lr_mult': 2}, 'schedular': {'sched': 'linear', 'lr': 1e-05, 'epochs': 5, \n",
    "'num_warmup_steps': 0.1}, 'start_eval': 0,'batch_size':5}\n",
    "# train_dataset, val_dataset, test_dataset = create_dataset('cosmos', config);\n",
    "# datasets = [train_dataset, val_dataset, test_dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/xvlm/lib/python3.9/site-packages/torchvision/transforms/transforms.py:834: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/xvlm/lib/python3.9/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = create_dataset('caption_cosmos', config);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.5441, -1.5149, -1.5149,  ..., -1.4127, -1.4127, -1.4127],\n",
       "          [-1.5441, -1.5441, -1.5441,  ..., -1.4127, -1.3981, -1.3981],\n",
       "          [-1.5733, -1.5587, -1.5879,  ..., -1.4127, -1.3981, -1.3981],\n",
       "          ...,\n",
       "          [-1.5149, -1.5149, -1.5149,  ..., -1.1207, -1.1207, -1.0769],\n",
       "          [-1.5149, -1.5149, -1.5149,  ..., -1.1937, -1.0915, -1.0477],\n",
       "          [-1.5149, -1.5149, -1.5149,  ..., -1.1937, -1.1791, -1.1353]],\n",
       " \n",
       "         [[-1.5120, -1.5120, -1.5120,  ..., -1.3919, -1.3919, -1.3919],\n",
       "          [-1.5120, -1.5120, -1.4820,  ..., -1.3769, -1.3769, -1.3769],\n",
       "          [-1.4970, -1.4970, -1.4669,  ..., -1.3769, -1.3619, -1.3619],\n",
       "          ...,\n",
       "          [-1.7371, -1.7371, -1.7371,  ...,  1.2495,  1.2194,  1.2044],\n",
       "          [-1.7371, -1.7371, -1.7371,  ...,  1.0994,  1.2645,  1.2945],\n",
       "          [-1.7371, -1.7371, -1.7371,  ...,  0.9793,  1.1444,  1.2495]],\n",
       " \n",
       "         [[-0.1720, -0.1293, -0.1435,  ...,  0.2404,  0.2404,  0.2404],\n",
       "          [-0.1435, -0.1435, -0.1578,  ...,  0.2262,  0.2546,  0.2546],\n",
       "          [-0.1435, -0.1293, -0.1862,  ...,  0.2120,  0.2404,  0.2404],\n",
       "          ...,\n",
       "          [-1.4660, -1.4660, -1.4660,  ..., -1.3665, -1.4091, -1.4091],\n",
       "          [-1.4660, -1.4660, -1.4660,  ..., -1.3807, -1.2954, -1.3096],\n",
       "          [-1.4660, -1.4660, -1.4660,  ..., -1.2811, -1.2954, -1.3380]]]),\n",
       " 'a picture of the technical infrastructure of whatsapp instagram and facebook messenger will be unified',\n",
       " 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_loader(datasets, [None, None, None], batch_size=[config['batch_size']] * 3,\n",
    "                                                          num_workers=[4, 4, 4], is_trains=[True, False, False],\n",
    "                                                          collate_fns=[None, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "torch.cuda.set_device(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading pretrained vision encoder\n",
      "### Loading pretrained text encoder\n",
      "load_capt_pretrain,  False\n",
      "### Loading pretrained text encoder\n",
      "load checkpoint from coco_capt_cider_step_44275.th\n",
      "missing_keys:  []\n",
      "unexpected_keys:  []\n"
     ]
    }
   ],
   "source": [
    "model_caption = XVLM_caption(config=config)\n",
    "model_caption.load_pretrained('coco_capt_cider_step_44275.th', config, is_eval=False)\n",
    "model_caption = model_caption.to('cuda');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a group of people sitting in front of a clock tower',\n",
       "  'a group of people sitting on the square with a statue',\n",
       "  'a group of people sitting on the plaza with a statue',\n",
       "  'a group of people sitting on the square with a clock tower',\n",
       "  'a group of people sitting on the steps with a statue'],\n",
       " tensor([-0.3649, -0.1935, -0.1850, -0.1832, -0.2098], device='cuda:2',\n",
       "        grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = test_dataset[361][0][None,:]\n",
    "image = image.to('cuda')\n",
    "torch.manual_seed(42)\n",
    "model_caption.generate(image,num_return_sequences=5,sample=True, num_beams=1,repetition_penalty=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [05:18<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device='cuda'\n",
    "captions = []\n",
    "for image, _, _, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    c = model_caption.generate(image,num_return_sequences=3,sample=True, num_beams=1,repetition_penalty=1.5)\n",
    "    cs = np.array_split(c[0],len(image))\n",
    "    cs = [list(cs) for cs in cs]\n",
    "    captions += cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a man in a leather jacket and a table',\n",
       " 'a man in a leather jacket and a lady with a table',\n",
       " 'a man in a leather jacket and a table']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[212]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('/root/thesis/ViLT/cosmos/test_data.json',orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43a2aefd9a4740e0597b3489c1c78fca90cd0ffb50c6fc4e233b22791010a45e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('xvlm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
